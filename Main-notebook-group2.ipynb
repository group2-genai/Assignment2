{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "714c8b9a-58ef-4bee-8afc-f1bf0feb6e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phase 1: Bronze Layer – Data Ingestion & Storage (1.5 Hours)\n",
    "- Load\traw\ttext\tfiles\t(TXT,\tJSON,\tCSV,\tor\tscraped\tdata)\tinto\tDatabricks\tDelta\tLake.\n",
    "- Store\tdata\tin\tthe\tBronze\tTable\twithout\tmodifications.\n",
    "- Log\tmetadata,\tfile\tschema,\tand\tperform\tbasic\tvalidation.\n",
    "- Apply\tbasic\tdata\tquality\tchecks\t(e.g.,\tmissing\tvalues,\tschema\tvalidation).\n",
    "- Create\ta\tdata\tingestion\tpipeline\tusing\tDatabricks\tAuto\tLoader\tor\tPySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b8a293-8a4b-4d6c-a524-797078c6b9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Uploaded the source csv file to Delta Lake as a table\n",
    "# This table can be read as Spark tabel\n",
    "news_df = spark.read.table(\"bbc_news\")\n",
    "\n",
    "bronze_table_path = \"/mnt/delta/bronze/bbc_news\"\n",
    "news_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_table_path)\n",
    "spark.read.format(\"delta\").load(\"/mnt/delta/bronze/bbc_news\").limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "274283f8-4b18-4142-a599-3f1cf7b1c1d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "news_df = spark.read.format(\"delta\").load(\"/mnt/delta/bronze/bbc_news\")\n",
    "news_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27173590-85d6-452b-b2ba-e188cf0497c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check null % count\n",
    "record_count = news_df.count()\n",
    "for column_name in news_df.columns:\n",
    "    missing_values = news_df.filter(col(column_name).isNull()).count()\n",
    "    print(f\"Missing values in {column_name} : {round((missing_values /record_count) * 100, 2) }% of total records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c98eb12-40dd-49ac-b417-cf5026da8c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phase 2: Silver Layer – Data Cleansing & AI Enrichment (2 Hours)\n",
    "\n",
    "•\tPerform\ttext\tpreprocessing\tsuch\tas\tlowercasing,\tpunctuation\tremoval,\ttokenization.\n",
    "•\tHandle\tstopwords,\tspecial\tcharacters,\tand\tformatting\tinconsistencies.\n",
    "•\tApply\tGPT-2\tfor\ttext\taugmentation\tincluding\tmissing\ttext\tgeneration,\treadability\timprovement,\tand\ttext\texpansion.\n",
    "•\tStore\tcleaned\tand\tAI-enhanced\tdata\tin\tthe\tSilver\tTable.\n",
    "•\tTrack\tdata\tlineage\tto\tshow\ttransformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d62d920a-a136-4718-aa8e-8e491273b619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lower, regexp_replace, split, trim, col\n",
    "from pyspark.sql.functions import array_remove\n",
    "from transformers import pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from transformers import pipeline\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Lowercasing\n",
    "news_df = news_df.withColumn(\"description\", lower(col(\"description\")))\n",
    "\n",
    "# Remove punctuation & special characters (excluding spaces)\n",
    "news_df = news_df.withColumn(\"description\", regexp_replace(col(\"description\"), \"[^\\w\\s]\", \"\"))\n",
    "\n",
    "# Remove multiple spaces and trim extra whitespace\n",
    "news_df = news_df.withColumn(\"description\", regexp_replace(col(\"description\"), \"\\s+\", \" \"))\n",
    "news_df = news_df.withColumn(\"description\", trim(col(\"description\")))\n",
    "\n",
    "# Tokenization (split on whitespace)\n",
    "news_df = news_df.withColumn(\"tokens\", split(col(\"description\"), \" \"))\n",
    "\n",
    "#stopword removal\n",
    "stopwords = [\"a\", \"an\", \"the\", \"is\", \"in\", \"on\", \"at\", \"and\", \"to\", \"of\", \"for\", \"with\", \"this\", \"that\", \"it\", \"as\", \"are\", \"was\", \"by\"]  \n",
    "\n",
    "for stopword in stopwords:\n",
    "    news_df = news_df.withColumn(\"tokens\", array_remove(col(\"tokens\"), stopword))\n",
    "\n",
    "# Reconstruct text after stopword removal\n",
    "news_df = news_df.withColumn(\"description\", regexp_replace(col(\"tokens\").cast(\"string\"), \"[\\[\\],']\", \"\"))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "news_df = news_df.drop(\"tokens\")\n",
    "\n",
    "# Display result\n",
    "display(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf971f4-3201-42e2-b4d6-4d1ab8a8fa9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 model once\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Function to augment text using GPT-2 with size restrictions\n",
    "def augment_text(text, max_length=50):  # Limit generated text length\n",
    "    input_text = text[:100]  # Limit input text length to 300 characters (if too long)\n",
    "    augmented = generator(input_text, max_length=max_length, num_return_sequences=1)\n",
    "    generated_text = augmented[0]['generated_text']\n",
    "    return generated_text[:50]  # Limit output to 500 characters\n",
    "\n",
    "# Register the function as a UDF\n",
    "augment_text_udf = udf(lambda text: augment_text(text), StringType())\n",
    "\n",
    "# Limit the number of rows to process and repartition into smaller batches\n",
    "limited_df = news_df.limit(100).repartition(10)  # Split into smaller partitions\n",
    "\n",
    "# Apply text augmentation to the 'description' column\n",
    "augmented_df = limited_df.withColumn('augmented_description', augment_text_udf(limited_df['description']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67970818-b5a9-4959-8e73-3259788b962e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(augmented_df['augmented_description'])\n",
    "silver_table_path = \"/mnt/delta/silver/bbc_news\"\n",
    "news_df.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576fc9af-7fb1-4d8d-994b-06ad85156feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phase 3: Gold Layer – Summarization & AI-Driven Insights (2.5 Hours)\n",
    "•\tApply\tGPT-2-based\tsummarization\tto\tgenerate\tkey\tinsights.\n",
    "•\tPerform\ttopic\tmodeling\t(LDA,\tBERT\tembeddings)\tfor\ttext\tclassification.\n",
    "•\tConduct\tsentiment\tanalysis.\n",
    "•\tGenerate\ttext\tembeddings\tfor\tAI-powered\tsearch.\n",
    "•\tStore\tfinal\tstructured\tdataset\tin\tthe\tGold\tTable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2805995d-57c7-4887-9cb1-b42d2533c6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pyspark.sql.functions as F\n",
    "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "\n",
    "def summary_text(text):\n",
    "    summary = summarizer(text, max_length=50, min_length=5, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Add a UDF to find summary of each of the description\n",
    "summary_udf = udf(summary_text, F.StringType())\n",
    "\n",
    "news_df = news_df.withColumn(\"summary\", summary_udf(F.col(\"description\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e7a399-c169-4ebe-89b9-bef7175649af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.driver.maxResultSize\", \"2g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca72303b-930e-4680-bb02-5d76d34dedf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Phase 4: Model Training & Fine-Tuning (1.5 Hours)\n",
    "# •\tFine-tune\tGPT-2\ton\tcustom.\n",
    "# •\tTrain\ta\tsimple\ttext\tclassification\tmodel\t(e.g.,\tclassifying\treviews,\ttopics).\n",
    "# •\tCompare\tpre-trained\tvs\tfine-tuned\tmodel\toutputs.\n",
    "# •\tSave\ttrained\tmodels\tin\tMLflow\tfor\ttracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716110c6-3726-4352-aac9-00ac0f538ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch spacy transformers datasets\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e10e5e-3a36-4369-b62b-07481ee72eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3d3498-2f69-4364-93c6-a74d594c3301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import spacy\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def clean_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    words = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Create small dataset\n",
    "data = [\n",
    "    {\"text\": \"I love this movie, it was fantastic!\", \"label\": 1},\n",
    "    {\"text\": \"The plot was boring and predictable.\", \"label\": 0},\n",
    "    {\"text\": \"Amazing cinematography and great acting!\", \"label\": 1},\n",
    "    {\"text\": \"Terrible script and poor character development.\", \"label\": 0},\n",
    "    {\"text\": \"A masterpiece! Highly recommended.\", \"label\": 1},\n",
    "    {\"text\": \"Worst movie I have ever seen.\", \"label\": 0}\n",
    "]\n",
    "\n",
    "# Clean dataset\n",
    "for item in data:\n",
    "    item[\"text\"] = clean_text(item[\"text\"])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Convert dataset to Hugging Face format\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_test = dataset.train_test_split(test_size=0.3)\n",
    "train_dataset, test_dataset = train_test[\"train\"], train_test[\"test\"]\n",
    "\n",
    "# Load Pre-trained GPT-2 model with classification head\n",
    "pretrained_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "pretrained_model.config.pad_token_id = pretrained_model.config.eos_token_id\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "finetuned_model.config.pad_token_id = finetuned_model.config.eos_token_id\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save fine-tuned model\n",
    "    torch.save(finetuned_model.state_dict(), \"gpt2_finetuned.pth\")\n",
    "    mlflow.pytorch.log_model(finetuned_model, \"gpt2_finetuned\")\n",
    "    \n",
    "    # Load pipelines\n",
    "    pretrained_classifier = pipeline(\"text-classification\", model=pretrained_model, tokenizer=tokenizer)\n",
    "    finetuned_classifier = pipeline(\"text-classification\", model=finetuned_model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Compare outputs\n",
    "    test_texts = [item[\"text\"] for item in data]\n",
    "    test_labels = [item[\"label\"] for item in data]\n",
    "    \n",
    "    pretrained_predictions = [int(p[\"label\"].split(\"_\")[-1]) for p in pretrained_classifier(test_texts)]\n",
    "    finetuned_predictions = [int(p[\"label\"].split(\"_\")[-1]) for p in finetuned_classifier(test_texts)]\n",
    "    \n",
    "    pretrained_accuracy = accuracy_score(test_labels, pretrained_predictions)\n",
    "    finetuned_accuracy = accuracy_score(test_labels, finetuned_predictions)\n",
    "    \n",
    "    print(\"Pretrained Model Accuracy:\", pretrained_accuracy)\n",
    "    print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)\n",
    "    \n",
    "    mlflow.log_metric(\"pretrained_accuracy\", pretrained_accuracy)\n",
    "    mlflow.log_metric(\"finetuned_accuracy\", finetuned_accuracy)\n",
    "    \n",
    "    print(\"Training complete. Model comparison done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a57dbf-b981-484b-a0e7-17ca3a90e318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def generate_business_insights_plots(df):\n",
    "    # 1. Word Cloud of Titles\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    title_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['cleaned_title']))\n",
    "    plt.imshow(title_wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Titles')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Word Cloud of Descriptions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    description_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['cleaned_description']))\n",
    "    plt.imshow(description_wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Descriptions')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Top 10 Most Frequent Words in Titles\n",
    "    title_words = ' '.join(df['cleaned_title']).split()\n",
    "    top_title_words = Counter(title_words).most_common(10)\n",
    "    top_title_words_df = pd.DataFrame(top_title_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Frequency', y='Word', data=top_title_words_df, palette='viridis')\n",
    "    plt.title('Top 10 Most Frequent Words in Titles')\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Top 10 Most Frequent Words in Descriptions\n",
    "    description_words = ' '.join(df['cleaned_description']).split()\n",
    "    top_description_words = Counter(description_words).most_common(10)\n",
    "    top_description_words_df = pd.DataFrame(top_description_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Frequency', y='Word', data=top_description_words_df, palette='viridis')\n",
    "    plt.title('Top 10 Most Frequent Words in Descriptions')\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Length Distribution of Titles\n",
    "    df['title_length'] = df['title'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['title_length'], kde=True, color='blue', bins=30)\n",
    "    plt.title('Length Distribution of Titles')\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Length Distribution of Descriptions\n",
    "    df['description_length'] = df['description'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['description_length'], kde=True, color='green', bins=30)\n",
    "    plt.title('Length Distribution of Descriptions')\n",
    "    plt.show()\n",
    "\n",
    "    # 7. Cosine similarity between the title and description for each article\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Vectorize the title and description columns\n",
    "    title_tfidf = vectorizer.fit_transform(df['title'])\n",
    "    description_tfidf = vectorizer.transform(df['description'])\n",
    "\n",
    "    # Compute cosine similarity between the title and description for each article\n",
    "    similarity_scores = cosine_similarity(title_tfidf, description_tfidf)\n",
    "\n",
    "    # Add the cosine similarity scores to the DataFrame\n",
    "    df['similarity_score'] = similarity_scores.diagonal()\n",
    "\n",
    "    # Plotting the distribution of similarity scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['similarity_score'], kde=True, color='blue', bins=10)\n",
    "    plt.title('Distribution of Cosine Similarity Scores between Titles and Descriptions')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the similarity scores in a box plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=df['similarity_score'], color='green')\n",
    "    plt.title('Box Plot of Cosine Similarity Scores between Titles and Descriptions')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26b47f51-f532-4239-8ef9-efc2c6d8ca8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phase 5: Visualization, Queries & Reporting (1.5 Hours)\n",
    "•\tQuery\tstructured\tinsights\tusing\tDatabricks\tSQL\t&\tPySpark.\n",
    "•\tBuild\ta\tbasic\tvisualization\tdashboard\tusing\tDatabricks\tNotebooks\t&\tPlotly.\n",
    "•\tValidate\tdata\tlineage\tfrom\tBronze\t→\tSilver\t→\tGold.\n",
    "•\tDiscuss\treal-world\tapplications\tof\tthis\tpipeline\t(e.g.,\tnews\tsummarization,\tchatbot\t\n",
    "training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51ed48cc-c0c0-4072-b6df-275bb283d178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All\tof\tthe\tbelow\tshould\tbe\tuploaded\tin\tthe\tgroup’s\tgithub\trepository.\t\n",
    "1.\tDatabricks\tNotebooks\tfor\teach\ttransformation\tphase.\n",
    "2.\tBronze,\tSilver,\tand\tGold\tDelta\tTables\twith\trespective\tdata\tstages (screenshots).\n",
    "3.\tAI-enriched,\tstructured\tinsights\tstored\tin\tGold\tLayer (screenshots).\n",
    "4.\tBasic\tML\tmodel\t&\tembeddings (print the\tmodel\tand\tembeddings\tin\ta\tfile)\n",
    "5.\tFinal\tqueries,\tdashboards,\tand\tvisualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8672a531-eb6d-4890-8c22-b3f3b62f3515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Main-notebook-group2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
